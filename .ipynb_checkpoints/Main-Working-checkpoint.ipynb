{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.y2014.VGG' from 'C:\\\\Users\\\\siddi\\\\Documents\\\\GitHub\\\\Convolutional-Architecture-Timeline\\\\models\\\\y2014\\\\VGG.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import datetime\n",
    "from importlib import reload\n",
    "import models\n",
    "from models.y2014 import VGG\n",
    "reload(models.y2014.VGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runinng on : cuda:0 \n",
      "PyTorch Verison : 1.5.0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torch.optim import Adam,RMSprop,SGD, lr_scheduler\n",
    "from torch.autograd import backward\n",
    "from torchvision.datasets import CIFAR100, CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torchvision\n",
    "print(\"Runinng on :\", device,\"\\nPyTorch Verison :\",torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_dataset = CIFAR10(\"./data/\",train=True,transform = transform,download=True)\n",
    "test_dataset = CIFAR10(\"./data/\",train=False, transform = transform,download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./runs/cifar10_experiment_1\")\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size 12500\n",
      "Test Size 2500\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Size\",len(train_dataloader))\n",
    "print(\"Test Size\",len(test_dataloader))\n",
    "dataloaders = {\"train\":train_dataloader, \"val\":test_dataloader}\n",
    "dataset_sizes = {\"train\":len(train_dataset), \"val\":len(test_dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Model Parameters: 4330122\n",
      "Total Trainable Parameters 4330122\n"
     ]
    }
   ],
   "source": [
    "model = VGG.VGG()\n",
    "model = model.to(device)\n",
    "\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"Total Model Parameters:\",pytorch_total_params)\n",
    "pytorch_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total Trainable Parameters\",pytorch_trainable_params)\n",
    "\n",
    "images,labels = iter(train_dataloader).next()\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "writer.add_image('One Batch of Images - CIFAR10'+str(labels), img_grid)\n",
    "\n",
    "writer.add_graph(model,  torch.rand((3,32,32)).unsqueeze(0).cuda())\n",
    "writer.close()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=5):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 100)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            print(datetime.datetime.now(),\"\\n\")\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            count = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                count = count +1 \n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                if count%100 == 0:\n",
    "                    print(\"-\"*40)\n",
    "                    print(\"STEP : \",count,\"/\",len(train_dataloader if phase==\"train\" else test_dataloader))\n",
    "                    print('ACC : {:.4f}'.format(running_corrects.double()/(count*batch_size)), phase.upper(),\"LOSS: {:.4f}\".format(running_loss / count*batch_size))\n",
    "                    if phase == \"train\": writer.add_scalar('training loss',running_loss / count*batch_size , epoch * len(train_dataloader) + count)\n",
    "                    if phase == \"train\": writer.add_scalar('training Accuracy',running_corrects.double() / count*batch_size , epoch * len(train_dataloader) + count)\n",
    "                    \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/29\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2020-06-13 17:53:36.984593 \n",
      "\n",
      "----------------------------------------\n",
      "STEP : 100 / 12500\n",
      "ACC :0.1725 TRAIN LOSS:36.5290\n",
      "----------------------------------------\n",
      "STEP : 200 / 12500\n",
      "ACC :0.1775 TRAIN LOSS:35.7210\n",
      "----------------------------------------\n",
      "STEP : 300 / 12500\n",
      "ACC :0.1808 TRAIN LOSS:34.9835\n",
      "----------------------------------------\n",
      "STEP : 400 / 12500\n",
      "ACC :0.1988 TRAIN LOSS:34.1944\n",
      "----------------------------------------\n",
      "STEP : 500 / 12500\n",
      "ACC :0.2125 TRAIN LOSS:33.7294\n",
      "----------------------------------------\n",
      "STEP : 600 / 12500\n",
      "ACC :0.2283 TRAIN LOSS:33.1808\n",
      "----------------------------------------\n",
      "STEP : 700 / 12500\n",
      "ACC :0.2421 TRAIN LOSS:32.5649\n",
      "----------------------------------------\n",
      "STEP : 800 / 12500\n",
      "ACC :0.2494 TRAIN LOSS:32.2692\n",
      "----------------------------------------\n",
      "STEP : 900 / 12500\n",
      "ACC :0.2558 TRAIN LOSS:31.9348\n",
      "----------------------------------------\n",
      "STEP : 1000 / 12500\n",
      "ACC :0.2630 TRAIN LOSS:31.5827\n",
      "----------------------------------------\n",
      "STEP : 1100 / 12500\n",
      "ACC :0.2693 TRAIN LOSS:31.2716\n",
      "----------------------------------------\n",
      "STEP : 1200 / 12500\n",
      "ACC :0.2744 TRAIN LOSS:30.9570\n",
      "----------------------------------------\n",
      "STEP : 1300 / 12500\n",
      "ACC :0.2798 TRAIN LOSS:30.6429\n",
      "----------------------------------------\n",
      "STEP : 1400 / 12500\n",
      "ACC :0.2852 TRAIN LOSS:30.4086\n",
      "----------------------------------------\n",
      "STEP : 1500 / 12500\n",
      "ACC :0.2938 TRAIN LOSS:30.1478\n",
      "----------------------------------------\n",
      "STEP : 1600 / 12500\n",
      "ACC :0.3016 TRAIN LOSS:29.8986\n",
      "----------------------------------------\n",
      "STEP : 1700 / 12500\n",
      "ACC :0.3088 TRAIN LOSS:29.6292\n",
      "----------------------------------------\n",
      "STEP : 1800 / 12500\n",
      "ACC :0.3126 TRAIN LOSS:29.4341\n",
      "----------------------------------------\n",
      "STEP : 1900 / 12500\n",
      "ACC :0.3182 TRAIN LOSS:29.2580\n",
      "----------------------------------------\n",
      "STEP : 2000 / 12500\n",
      "ACC :0.3212 TRAIN LOSS:29.1233\n",
      "----------------------------------------\n",
      "STEP : 2100 / 12500\n",
      "ACC :0.3242 TRAIN LOSS:28.9959\n",
      "----------------------------------------\n",
      "STEP : 2200 / 12500\n",
      "ACC :0.3298 TRAIN LOSS:28.8333\n",
      "----------------------------------------\n",
      "STEP : 2300 / 12500\n",
      "ACC :0.3330 TRAIN LOSS:28.7280\n",
      "----------------------------------------\n",
      "STEP : 2400 / 12500\n",
      "ACC :0.3371 TRAIN LOSS:28.6050\n",
      "----------------------------------------\n",
      "STEP : 2500 / 12500\n",
      "ACC :0.3402 TRAIN LOSS:28.4779\n",
      "----------------------------------------\n",
      "STEP : 2600 / 12500\n",
      "ACC :0.3432 TRAIN LOSS:28.3422\n",
      "----------------------------------------\n",
      "STEP : 2700 / 12500\n",
      "ACC :0.3478 TRAIN LOSS:28.1998\n",
      "----------------------------------------\n",
      "STEP : 2800 / 12500\n",
      "ACC :0.3506 TRAIN LOSS:28.0734\n",
      "----------------------------------------\n",
      "STEP : 2900 / 12500\n",
      "ACC :0.3547 TRAIN LOSS:27.8891\n",
      "----------------------------------------\n",
      "STEP : 3000 / 12500\n",
      "ACC :0.3571 TRAIN LOSS:27.7951\n",
      "----------------------------------------\n",
      "STEP : 3100 / 12500\n",
      "ACC :0.3606 TRAIN LOSS:27.6422\n",
      "----------------------------------------\n",
      "STEP : 3200 / 12500\n",
      "ACC :0.3639 TRAIN LOSS:27.5218\n",
      "----------------------------------------\n",
      "STEP : 3300 / 12500\n",
      "ACC :0.3668 TRAIN LOSS:27.3930\n",
      "----------------------------------------\n",
      "STEP : 3400 / 12500\n",
      "ACC :0.3694 TRAIN LOSS:27.2795\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-233d7e6dfd9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_ft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-092de33e6271>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     39\u001b[0m                     \u001b[1;31m# backward + optimize only if in training phase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gans\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gans\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
